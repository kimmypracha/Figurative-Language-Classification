{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_metric\n",
    "import wandb\n",
    "import os\n",
    "import evaluate\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define the dataset class to process with the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "  def __init__(self, filename=None, df=None, tokenizer=None):\n",
    "    super().__init__()\n",
    "    if filename:\n",
    "      self.df = pd.read_csv(filename, sep=\"\\t\")\n",
    "    else:\n",
    "      self.df = df\n",
    "    self.encoding = tokenizer(self.df[\"text\"].values.tolist(), truncation=True, padding=True, return_tensors=\"pt\")\n",
    "  def __len__(self):\n",
    "    return len(self.df)\n",
    "  def __getitem__(self, index):\n",
    "    label = 1 if self.df[\"label\"].iloc[index] == \"MOS\" else 0\n",
    "    return {\"input_ids\": self.encoding[\"input_ids\"][index], \"attention_mask\" : self.encoding[\"attention_mask\"][index], \"label\" : label}\n",
    "  def getlabels(self):\n",
    "    return np.array([1 if label == \"MOS\" else 0 for label in self.df[\"label\"].values])\n",
    "  def getDataFrame(self):\n",
    "    return self.df\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "  def __init__(self, filename=None, df=None, tokenizer=None):\n",
    "    super().__init__()\n",
    "    if filename:\n",
    "      self.df = pd.read_csv(filename, sep=\"\\t\")\n",
    "    else:\n",
    "      self.df = df\n",
    "    self.encoding = tokenizer(self.df[\"text\"].values.tolist(), truncation=True, padding=True, return_tensors=\"pt\")\n",
    "  def __len__(self):\n",
    "    return len(self.df)\n",
    "  def __getitem__(self, index):\n",
    "    return {\"input_ids\": self.encoding[\"input_ids\"][index], \"attention_mask\" : self.encoding[\"attention_mask\"][index]}\n",
    "  def getlabels(self):\n",
    "    return np.array([1 if label == \"MOS\" else 0 for label in self.df[\"label\"].values])\n",
    "  def getDataFrame(self):\n",
    "    return self.df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Initialize the model and the tokenizer \n",
    "\n",
    "    In this case, we have the model choice as the following:\n",
    "\n",
    "        1. bert-base-uncased (BERT)\n",
    "\n",
    "        2. roberta-base (RoBERTa)\n",
    "\n",
    "        3. distilgpt2 (Distil-GPT2)\n",
    "\n",
    "    We can run the cell below multiple times and change the model_name to try different model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "def model_init():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name,num_labels=2)\n",
    "    # model.config.pad_token_id = model.config.eos_token_id # use this if the model is GPT\n",
    "    return model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# tokenizer.pad_token = tokenizer.eos_token # use this if the model is GPT\n",
    "\n",
    "train_ds = TrainDataset(\"train.tsv\", tokenizer=tokenizer)\n",
    "validate_ds = TrainDataset(\"dev.tsv\", tokenizer=tokenizer)\n",
    "test_ds = TestDataset(\"test.tsv\", tokenizer=tokenizer)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Search\n",
    "\n",
    "    To find the best configuration for each model, we have to do the hyperparameter search. However, trying every possible value to find the best model is computationally infeasible. We decide to use random search on 20 samples to find the best parameter config for the model based on the evaluation accuracy and the loss between training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random'\n",
    "}\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "parameters_dict = {\n",
    "    'epochs': {\n",
    "        'values': [1,2,3,4,5]\n",
    "        },\n",
    "    'batch_size': {\n",
    "        'value': 8\n",
    "        },\n",
    "    'learning_rate': {\n",
    "        'distribution': 'log_uniform_values',\n",
    "        'min': 1e-5,\n",
    "        'max': 1e-3\n",
    "    },\n",
    "    'weight_decay': {\n",
    "        'values': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project='annotation-project-sweeps')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the metric that we will use are accuracy, recall, precision, and F1 score. Since we would like to know every aspect of the model prediction so that we can decide the one that has the best balance in all measure later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_fn(eval_preds):\n",
    "  metrics_data = dict()\n",
    "  \n",
    "  accuracy_metric = load_metric('accuracy')\n",
    "  precision_metric = load_metric('precision')\n",
    "  recall_metric = load_metric('recall')\n",
    "  f1_metric = load_metric('f1')\n",
    "\n",
    "\n",
    "  logits = eval_preds.predictions\n",
    "  labels = eval_preds.label_ids\n",
    "  preds = np.argmax(logits, axis=-1)  \n",
    "  \n",
    "  metrics_data.update(accuracy_metric.compute(predictions=preds, references=labels))\n",
    "  metrics_data.update(precision_metric.compute(predictions=preds, references=labels, average='weighted'))\n",
    "  metrics_data.update(recall_metric.compute(predictions=preds, references=labels, average='weighted'))\n",
    "  metrics_data.update(f1_metric.compute(predictions=preds, references=labels, average='weighted'))\n",
    "  return metrics_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=None):\n",
    "  with wandb.init(config=config):\n",
    "    # set sweep configuration\n",
    "    config = wandb.config\n",
    "\n",
    "\n",
    "    # set training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='ap-sweeps',\n",
    "\t      report_to='wandb',  # Turn on Weights & Biases logging\n",
    "        num_train_epochs=config.epochs,\n",
    "        learning_rate=config.learning_rate,\n",
    "        weight_decay=config.weight_decay,\n",
    "        per_device_train_batch_size=config.batch_size,\n",
    "        per_device_eval_batch_size=16,\n",
    "        save_strategy='epoch',\n",
    "        evaluation_strategy='epoch',\n",
    "        logging_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "        remove_unused_columns=False,\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "\n",
    "    # define training loop\n",
    "    trainer = Trainer(\n",
    "        # model,\n",
    "        model_init=model_init,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=validate_ds,\n",
    "        compute_metrics=compute_metrics_fn\n",
    "    )\n",
    "\n",
    "\n",
    "    # start training loop\n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "wandb.agent(sweep_id, train, count=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model with the best config from hyperparameter search\n",
    "\n",
    "    In the config value below, we can change to the other value based on the model that we would like to train and replicate the accuracy from the hyperparameter tuning. After training, the model will be saved with the format [model_name]-lr[learning_rate]-bs[batch_size]-ep[num_epoch]-wd[weight_decay]. We can load the model with that name for inference later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    \"learning_rate\" : 5.315e-5,\n",
    "    \"weight_decay\" : 0.3,\n",
    "    \"batch_size\" : 8,\n",
    "    \"epoch\" : 3\n",
    "}\n",
    "config_name =\"{}-lr{}-bs{}-ep{}-wd{}\".format(model_name, config[\"learning_rate\"], config[\"batch_size\"], config[\"epoch\"], config[\"weight_decay\"])\n",
    "training_args = TrainingArguments(output_dir=\"model/\", \n",
    "                                 evaluation_strategy=\"epoch\", \n",
    "                                 per_device_train_batch_size=config[\"batch_size\"],\n",
    "                                 per_device_eval_batch_size=config[\"batch_size\"],\n",
    "                                 learning_rate=config[\"learning_rate\"],\n",
    "                                 weight_decay=config[\"weight_decay\"],\n",
    "                                 num_train_epochs=config[\"epoch\"],\n",
    "                                 lr_scheduler_type=\"linear\",\n",
    "                                 warmup_ratio=0,\n",
    "                                 warmup_steps=0,\n",
    "                                 log_level=\"passive\",\n",
    "                                 logging_strategy=\"epoch\",\n",
    "                                #  eval_steps=50,\n",
    "                                 fp16=True,\n",
    "                                 run_name=config_name,\n",
    "                                 report_to=\"wandb\"\n",
    "                                 \n",
    ")\n",
    "\n",
    "trainer = Trainer(model_init=model_init,\n",
    "                 args=training_args,\n",
    "                 train_dataset=train_ds,\n",
    "                 eval_dataset=validate_ds,\n",
    "                 compute_metrics=compute_metrics\n",
    "                 )\n",
    "                                 \n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(config_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased-lr5e-05-ep8-wd2')\n",
    "trainer = Trainer(model=train_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[107  12]\n",
      " [ 14  67]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10224/1756225463.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  y_preds = torch.argmax(F.softmax(predictions), axis=-1).cpu().detach().numpy()\n"
     ]
    }
   ],
   "source": [
    "y_trues = test_ds.getlabels()\n",
    "predictions = torch.from_numpy(trainer.predict(test_ds).predictions).float()\n",
    "y_preds = torch.argmax(F.softmax(predictions), axis=-1).cpu().detach().numpy()\n",
    "confusion_matrix = metrics.confusion_matrix(y_trues, y_preds, labels=[0, 1]) # NMOS, MOS\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see that the confusion matrix are the following :\n",
    "|   | Predicted NMOS | Predicted MOS |\n",
    "|----|--------------|----------------|\n",
    "| Actual NMOS | 107 | 12 |\n",
    "| Actual MOS  | 14  | 67 |\n",
    "\n",
    "Now we can try to see the number of book that is mislabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adjudicator_id\n",
       "dung          3\n",
       "jtopa         4\n",
       "phazarika     3\n",
       "ppromthaww    9\n",
       "tranguyen     7\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = test_ds.getDataFrame()\n",
    "test_df[\"pred\"] = y_preds\n",
    "test_df[\"pred_label\"] = test_df[\"pred\"].apply(lambda x: \"MOS\" if x == 1 else \"NMOS\")\n",
    "disagree = test_df[test_df.label != test_df.pred_label]\n",
    "disagree.groupby(\"adjudicator_id\").count()[\"label\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This mean that the number of mislabeled from lowest to highest is the following : \n",
    "1. The Lost World (1912) (3)\n",
    "2. The Moon Maid (1926) (3)\n",
    "3. A Room with a View (4)\n",
    "4. Dorothy and the Wizard in Oz (7)\n",
    "4. White Fangs (9) \n",
    "\n",
    "We can sample the mislabeled text with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = disagree.sample(10)\n",
    "sample.to_csv(\"sample.tsv\",sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More detail in the inference notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Aug 10 2022, 11:40:04) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
